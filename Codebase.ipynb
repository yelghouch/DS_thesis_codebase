{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b306ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "807d2bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patient_selection(admissions_csv, patients_csv, notes_csv):\n",
    "    #loading the MIMIC tables as dataframes\n",
    "    admissions = pd.read_csv(admissions_csv)\n",
    "    patients = pd.read_csv(patients_csv)\n",
    "    notes = pd.read_csv(notes_csv)\n",
    "    \n",
    "    #dropping unnecessary columns\n",
    "    admissions = admissions.drop(['ROW_ID', 'LANGUAGE', 'MARITAL_STATUS', 'RELIGION',\\\n",
    "                              'ETHNICITY', 'EDREGTIME', 'EDOUTTIME',\\\n",
    "                              'HOSPITAL_EXPIRE_FLAG', 'HAS_CHARTEVENTS_DATA',\\\n",
    "                              'DISCHARGE_LOCATION'], axis=1)\n",
    "    patients = patients.drop(['ROW_ID', 'DOD', 'DOD_HOSP', \\\n",
    "                              'DOD_SSN', 'EXPIRE_FLAG'], axis =1)\n",
    "    notes = notes.drop(['ROW_ID', 'STORETIME', 'DESCRIPTION', 'CGID'], axis=1)\n",
    "\n",
    "    #dropping unnecessary rows\n",
    "    notes = notes[notes.CATEGORY != 'Discharge summary']\n",
    "    notes = notes[notes.ISERROR != 1]\n",
    "    admissions.groupby('SUBJECT_ID')\n",
    "    admissions = admissions.drop_duplicates(subset='SUBJECT_ID', keep=False)\n",
    "\n",
    "    \n",
    "    ############################\n",
    "    ### combining the tables ###\n",
    "    ############################\n",
    "    \n",
    "    \n",
    "    #merging notes and admissions + dropping unusuable notes\n",
    "    notes_adm = pd.merge(notes, admissions, on=\"HADM_ID\", how=\"left\")\n",
    "    notes_adm = notes_adm[notes_adm['HADM_ID'].notna()]\n",
    "    notes_adm = notes_adm[notes_adm['CHARTTIME'].notna()]\n",
    "    \n",
    "    #converting the timedata to usable format\n",
    "    notes_adm['CHARTDATE'] = pd.to_datetime(notes_adm['CHARTDATE'], format='%Y-%m-%d %H:%M:%S')\n",
    "    notes_adm['CHARTTIME'] = pd.to_datetime(notes_adm['CHARTTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "    notes_adm['ADMITTIME'] = pd.to_datetime(notes_adm['ADMITTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    #filtering out the notes that are needed\n",
    "    notes_adm = notes_adm.loc[notes_adm.CHARTTIME >= notes_adm.ADMITTIME]\n",
    "    notes_24h = notes_adm[(notes_adm.CHARTTIME - notes_adm.ADMITTIME <= timedelta(hours=24)) & (notes_adm.CHARTTIME > notes_adm.ADMITTIME)]\n",
    "\n",
    "    patients['DOB'] = pd.to_datetime(patients['DOB'], format='%Y-%m-%d %H:%M:%S')\n",
    "    notes_24h = notes_24h.rename(columns={'SUBJECT_ID_x': 'SUBJECT_ID'})\n",
    "    notes_24h = pd.merge(notes_24h, patients, on=\"SUBJECT_ID\", how=\"left\")\n",
    "    notes_24h = notes_24h.drop(['SUBJECT_ID_y'], axis =1)\n",
    "    notes_24h = notes_24h.assign(AGE = lambda x: (x['ADMITTIME']).astype('int64') - (x['DOB']).astype('int64'))\n",
    "    notes_24h.AGE = notes_24h.AGE.apply(lambda x: int((((x*1e-9))/(60*60*24*365.242))))\n",
    "    notes_24h.AGE = notes_24h.AGE.abs()\n",
    "    \n",
    "    bins = [-1, 17, 34, 49, 64, 89, np.inf]\n",
    "    cat = ['0-17', '18-34', '35-49', '50-64', '65-89', '90+']\n",
    "    notes_24h['AGERANGE'] = pd.cut(notes_24h['AGE'], bins, labels=cat)\n",
    "    notes_24h = notes_24h[notes_24h.AGERANGE != '0-17']\n",
    "    \n",
    "    #creating binary mortality feature\n",
    "    notes_24h['MORTALITY'] = notes_24h['DEATHTIME'].replace(np.nan, 0)\n",
    "    notes_24h['MORTALITY'] = np.where(notes_24h['MORTALITY'] != 0.0, 1, 0)\n",
    "    notes_24h = notes_24h.drop('DEATHTIME', 1)\n",
    "    \n",
    "    return notes_24h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c064af",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_24h = patient_selection('admissions.csv', 'patients.csv', 'notes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c112e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncation(notes):\n",
    "    notes.sort_values(by=['SUBJECT_ID', 'CHARTTIME'], ascending = True)\n",
    "    g = notes.groupby('SUBJECT_ID')\n",
    "    notes_fl = (pd.concat([g.head(1), g.tail(1)]).drop_duplicates().sort_values('SUBJECT_ID').reset_index(drop=True))\n",
    "    notes = notes_f1\n",
    "    \n",
    "    notes.TEXT = notes.TEXT.map(lambda x: x[1:] if x[0] == ' ' else x)\n",
    "    notes.TEXT = notes.TEXT.map(lambda x: x[:-1] if x[-1] == ' ' else x)\n",
    "    \n",
    "    nlp = spacy.load('en_core_sci_lg')\n",
    "    with nlp.select_pipes(disable = ['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner']):\n",
    "        spacy_tok = nlp.tokenizer\n",
    "    \n",
    "    #creating tokens based on spacy tokenizer\n",
    "    def collect_tokens(text):\n",
    "    tokens = []\n",
    "    for token in spacy_tok(text):\n",
    "        tokens.append(token.text)\n",
    "    return tokens\n",
    "\n",
    "    notes['sp_tok'] = notes.TEXT.map(collect_tokens)\n",
    "    notes['num_sp_tok'] = notes.sp_tok.apply(lambda x: len(x))\n",
    "    \n",
    "    \n",
    "    # creating tokens based on spacy tokenizer without stopwords\n",
    "    def tokens_nostop(text):\n",
    "        tokens = []\n",
    "        for token in spacy_tok(text):\n",
    "            if token.is_stop == False:\n",
    "                tokens.append(token.text)\n",
    "        return tokens\n",
    "\n",
    "    notes['sp_nostop_tok'] = notes.TEXT.map(tokens_nostop)\n",
    "    notes['num_sp_nostop_tok'] = notes.sp_nostop_tok.apply(lambda x: len(x))\n",
    "    \n",
    "    MAX_SIZE_NOTE = np.percentile(num_spacy, 90)\n",
    "    MAX_SIZE_NOTE_ns = np.percentile(num_nostop, 90)\n",
    "\n",
    "    # truncating the notes\n",
    "    notes['sp_tok_trun'] = notes.sp_tok.apply(lambda x: x[0:MAX_SIZE_NOTE])\n",
    "    notes['n_sp_tok_trun'] = notes.sp_tok_trun.apply(lambda x: len(x))\n",
    "    notes['sp_nostop_tok_trun'] = notes.sp_nostop_tok.apply(lambda x: x[0:MAX_SIZE_NOTE_ns])\n",
    "    notes['n_sp_nostop_tok_trun'] = notes.sp_nostop_tok_trun.apply(lambda x: len(x))\n",
    "    \n",
    "    notes['TEXT_sp_trun'] = notes.sp_tok_trun.apply(lambda x: \" \".join(x))\n",
    "    notes['TEXT_sp_nostop_trun'] = notes.sp_nostop_tok_trun.apply(lambda x: \" \".join(x))\n",
    "    \n",
    "    notes_sp_trun = notes.groupby(['SUBJECT_ID'], as_index=False).agg({'TEXT_sp_trun': lambda x: '\\n\\n\\n newnote \\n\\n\\n'.join(x),'sp_tok_trun': 'sum'})\n",
    "    notes_sp_trun['NUM_sp_trun'] = notes_sp_trun.sp_tok_trun.apply(lambda x: len(x))\n",
    "    notes_sp_trun.rename(columns = {'sp_tok_trun':'TOK_sp_trun'}, inplace = True)\n",
    "    \n",
    "    notes_sp_nostop_trun = notes.groupby(['SUBJECT_ID'], as_index=False).agg({'TEXT_sp_nostop_trun': lambda x: '\\n\\n\\n newnote \\n\\n\\n'.join(x),'sp_nostop_tok_trun': 'sum'})\n",
    "    notes_sp_nostop_trun['NUM_sp_nostop_trun'] = notes_sp_nostop_trun.sp_nostop_tok_trun.apply(lambda x: len(x))\n",
    "    notes_sp_nostop_trun.rename(columns = {'sp_nostop_tok_trun':'TOK_sp_nostop_trun'}, inplace = True)\n",
    "    \n",
    "    not_notes = notes[['SUBJECT_ID', 'HADM_ID', 'ADMITTIME', 'ADMISSION_TYPE', 'ADMISSION_LOCATION', 'INSURANCE',\\\n",
    "                      'DIAGNOSIS', 'GENDER', 'DOB', 'AGE','AGERANGE', 'MORTALITY'] ]\n",
    "    not_notes = not_notes.drop_duplicates(subset=['SUBJECT_ID'], keep='last')\n",
    "    \n",
    "    notes_sp_trun = notes_sp_trun.merge(not_notes, how='left', on='SUBJECT_ID')\n",
    "    notes_sp_nostop_trun = notes_sp_nostop_trun.merge(not_notes, how='left', on='SUBJECT_ID')\n",
    "\n",
    "    return notes_sp_trun, notes_sp_nostop_trun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01164a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_trun, notes_nostop_trun = truncation(notes_24h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa31d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncation_nopunc(notes):\n",
    "    notes.sort_values(by=['SUBJECT_ID', 'CHARTTIME'], ascending = True)\n",
    "    g = notes.groupby('SUBJECT_ID')\n",
    "    notes_fl = (pd.concat([g.head(1), g.tail(1)]).drop_duplicates().sort_values('SUBJECT_ID').reset_index(drop=True))\n",
    "    notes = notes_fl\n",
    "    \n",
    "    def preprocess_text(i):\n",
    "        reject_punc = '''!\"#$%&'()-*+/;<=>?@\\^_`{|}~,.[]:'''\n",
    "        i = i.lower().translate(str.maketrans(dict.fromkeys(reject_punc, ' ')))\n",
    "        i = re.sub(r' \\n', ' ', i)\n",
    "        i = re.sub(r' \\r', ' ', i)\n",
    "        return i\n",
    "    notes.TEXT = notes.TEXT.map(preprocess_text)\n",
    "\n",
    "    notes.TEXT = notes.TEXT.map(lambda x: x[1:] if x[0] == ' ' else x)\n",
    "    notes.TEXT = notes.TEXT.map(lambda x: x[:-1] if x[-1] == ' ' else x)\n",
    "    \n",
    "    nlp = spacy.load('en_core_sci_lg')\n",
    "    with nlp.select_pipes(disable = ['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner']):\n",
    "        spacy_tok = nlp.tokenizer\n",
    "        \n",
    "    # creating tokens based on spacy tokenizer\n",
    "    def collect_tokens(text):\n",
    "        tokens = []\n",
    "        for token in spacy_tok(text):\n",
    "            tokens.append(token.text)\n",
    "        return tokens\n",
    "\n",
    "    notes['sp_tok'] = notes.TEXT.map(collect_tokens)\n",
    "    notes['num_sp_tok'] = notes.sp_tok.apply(lambda x: len(x))\n",
    "    \n",
    "    # creating tokens based on spacy tokenizer without stopwords\n",
    "    def tokens_nostop(text):\n",
    "        tokens = []\n",
    "        for token in spacy_tok(text):\n",
    "            if token.is_stop == False:\n",
    "                tokens.append(token.text)\n",
    "        return tokens\n",
    "\n",
    "    notes['sp_nostop_tok'] = notes.TEXT.map(tokens_nostop)\n",
    "    notes['num_sp_nostop_tok'] = notes.sp_nostop_tok.apply(lambda x: len(x))\n",
    "    \n",
    "    MAX_SIZE_NOTE = np.percentile(num_spacy, 90)\n",
    "    MAX_SIZE_NOTE_ns = np.percentile(num_nostop, 90)\n",
    "    \n",
    "    # truncating the notes\n",
    "    notes['sp_tok_trun'] = notes.sp_tok.apply(lambda x: x[0:MAX_SIZE_NOTE])\n",
    "    notes['n_sp_tok_trun'] = notes.sp_tok_trun.apply(lambda x: len(x))\n",
    "    notes['sp_nostop_tok_trun'] = notes.sp_nostop_tok.apply(lambda x: x[0:MAX_SIZE_NOTE_ns])\n",
    "    notes['n_sp_nostop_tok_trun'] = notes.sp_nostop_tok_trun.apply(lambda x: len(x))\n",
    "    \n",
    "    notes['TEXT_sp_trun'] = notes.sp_tok_trun.apply(lambda x: \" \".join(x))\n",
    "    notes['TEXT_sp_nostop_trun'] = notes.sp_nostop_tok_trun.apply(lambda x: \" \".join(x))\n",
    "    notes_sp_trun = notes.groupby(['SUBJECT_ID'], as_index=False).agg({'TEXT_sp_trun': lambda x: '\\n\\n\\n newnote \\n\\n\\n'.join(x),'sp_tok_trun': 'sum'})\n",
    "    notes_sp_trun['NUM_sp_trun'] = notes_sp_trun.sp_tok_trun.apply(lambda x: len(x))\n",
    "    notes_sp_trun.rename(columns = {'sp_tok_trun':'TOK_sp_trun'}, inplace = True)\n",
    "    notes_sp_nostop_trun = notes.groupby(['SUBJECT_ID'], as_index=False).agg({'TEXT_sp_nostop_trun': lambda x: '\\n\\n\\n newnote \\n\\n\\n'.join(x),'sp_nostop_tok_trun': 'sum'})\n",
    "    notes_sp_nostop_trun['NUM_sp_nostop_trun'] = notes_sp_nostop_trun.sp_nostop_tok_trun.apply(lambda x: len(x))\n",
    "    notes_sp_nostop_trun.rename(columns = {'sp_nostop_tok_trun':'TOK_sp_nostop_trun'}, inplace = True)\n",
    "    not_notes = notes[['SUBJECT_ID', 'HADM_ID', 'ADMITTIME', 'ADMISSION_TYPE', 'ADMISSION_LOCATION', 'INSURANCE',\\\n",
    "                      'DIAGNOSIS', 'GENDER', 'DOB', 'AGE','AGERANGE', 'MORTALITY'] ]\n",
    "    not_notes = not_notes.drop_duplicates(subset=['SUBJECT_ID'], keep='last')\n",
    "    \n",
    "    notes_sp_trun = notes_sp_trun.merge(not_notes, how='left', on='SUBJECT_ID')\n",
    "    notes_sp_nostop_trun = notes_sp_nostop_trun.merge(not_notes, how='left', on='SUBJECT_ID')\n",
    "    \n",
    "    return notes_sp_trun, notes_sp_nostop_trun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca7095",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_trun_nopunc, notes_nostop_trun_nopunc = truncation_nopunc(notes_24h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f8304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def note_selector(notes):\n",
    "    #separating dying and surviving patients\n",
    "    pos_notes = notes.loc[notes.MORTALITY == 1]\n",
    "    neg_notes = notes.loc[notes.MORTALITY == 0]\n",
    "    \n",
    "    #sampling pos and neg\n",
    "    pos_notes_rand = pos_notes.sample(n=25)\n",
    "    neg_notes_rand = neg_notes.sample(n=25)\n",
    "    \n",
    "    #combining pos and neg + shuffling and index reset\n",
    "    notes_physician = pd.concat([pos_notes_rand, neg_notes_rand])\n",
    "    notes_physician = notes_physician.sample(frac=1).reset_index(drop=True) \n",
    "    \n",
    "    return notes_physician"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2938327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_physician = note_selector(notes_trun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1b9fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select a vectorizer by commenting the others\n",
    "def vectorizer(notes):    \n",
    "    ######################\n",
    "    ### TF-IDF Vectors ###\n",
    "    ######################\n",
    "    vect = TfidfVectorizer(lowercase=True, max_features=20000)\n",
    "    wm = vect.fit_transform(notes.TOK_sp_trun.values)\n",
    "    vect_tokens = vect.get_feature_names()\n",
    "    \n",
    "    #####################\n",
    "    ### Count Vectors ###\n",
    "    #####################\n",
    "#     vect = CountVectorizer(lowercase=True, max_features=20000)\n",
    "#     wm = vect.fit_transform(notes.TOK_sp_trun.values)\n",
    "#     vect_tokens = vect.get_feature_names()\n",
    "    \n",
    "    ######################\n",
    "    ### Binary Vectors ###\n",
    "    ######################\n",
    "#     vect = CountVectorizer(binary=True, lowercase=True, max_features=20000)\n",
    "#     wm = vect.fit_transform(notes.TOK_sp_trun.values)\n",
    "#     vect_tokens = vect.get_feature_names()\n",
    "\n",
    "    return wm, vect_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ac008",
   "metadata": {},
   "outputs": [],
   "source": [
    "wm, vect_tokens = vectorizer(notes_trun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a1691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_prep(notes, batch1, wm):\n",
    "    batch1 = batch1.drop(['HADM_ID', 'ADMITTIME', 'ADMISSION_TYPE', 'ADMISSION_LOCATION', 'INSURANCE', 'DIAGNOSIS', 'DOB', 'AGE'], axis=1)\n",
    "    subjects1 = list(batch1.SUBJECT_ID.values)\n",
    "    \n",
    "    labels = notes.MORTALITY.values\n",
    "    subjects = notes.SUBJECT_ID.values\n",
    "    \n",
    "    #making the vectors usable as input for a model\n",
    "    def vec_processing(wm):\n",
    "        input_vecs = []\n",
    "        for i in wm:\n",
    "            input_vecs.append(i.toarray().tolist())\n",
    "\n",
    "        def flatten(l):\n",
    "            flat_list = []\n",
    "            for sublist in l:\n",
    "                for item in sublist:\n",
    "                    flat_list.append(item)\n",
    "            return flat_list\n",
    "\n",
    "        flat_vecs = []\n",
    "        for i in tqdm(input_vecs):\n",
    "            flat_i = flatten(i)\n",
    "            flat_vecs.append(flat_i)\n",
    "        return flat_vecs\n",
    "\n",
    "    flat_vecs = vec_processing(wm)\n",
    "    \n",
    "    data = pd.DataFrame()\n",
    "    data['subjects'] = subjects\n",
    "    data['vecs']  = flat_vecs\n",
    "    data['labels'] = labels\n",
    "\n",
    "    batch1_data = data[data.subjects.isin(subjects1)]\n",
    "    data = data[~data.subjects.isin(subjects1)]\n",
    "    \n",
    "    #train and test set\n",
    "    data = data.sample(n=len(data), random_state=25)\n",
    "    data = data.reset_index(drop = True)\n",
    "    data_test = data.sample(frac=0.2, random_state=25)\n",
    "    data_train_all = data.drop(data_test.index)\n",
    "    \n",
    "    #undersampling the negatives in training\n",
    "    rows_pos = data_train_all.labels == 1\n",
    "    train_pos = data_train_all.loc[rows_pos]\n",
    "    train_neg = data_train_all.loc[-rows_pos]\n",
    "\n",
    "    #merge balance\n",
    "    data_train = pd.concat([train_pos, train_neg.sample(n=len(train_pos), random_state=25)], axis=0)\n",
    "    data_train = data_train.sample(n=len(data_train), random_state=25).reset_index(drop=True)\n",
    "    \n",
    "    return data_train, data_test, batch1_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ce786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, batch1_data = training_prep(notes, batch1, wm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98162729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(data_train, data_test):\n",
    "    #prepare data\n",
    "    X_train_usc, y_train = data_train.vecs.values, data_train.labels.values\n",
    "    n_samples = len(data_train.vecs.values)\n",
    "    n_features = len(tfidf_tokens)\n",
    "\n",
    "    X_test_usc, y_test = data_test.vecs.values, data_test.labels.values\n",
    "\n",
    "    X_train = np.array([np.array(xi) for xi in X_train_usc])\n",
    "    X_test = np.array([np.array(xi) for xi in X_test_usc])\n",
    "\n",
    "    X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "    X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "    y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "    y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "    y_train = y_train.view(y_train.shape[0],1)\n",
    "    y_test = y_test.view(y_test.shape[0],1)\n",
    "\n",
    "    # 1) model\n",
    "    class LogisticRegression(nn.Module):\n",
    "        def __init__(self, n_input_features):\n",
    "            super(LogisticRegression, self).__init__()\n",
    "            self.linear = nn.Linear(n_input_features, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            y_predicted = torch.sigmoid(self.linear(x))\n",
    "            return y_predicted\n",
    "\n",
    "    model = LogisticRegression(n_features)\n",
    "\n",
    "    # 2) loss and optimizer\n",
    "    lr = 0.01\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # 3) training loop\n",
    "    num_epochs = 10000\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        y_predicted = model(X_train)\n",
    "        loss = criterion(y_predicted, y_train)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (epoch+1) % 1000 == 0:\n",
    "            print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "            \n",
    "    \n",
    "    # 4) testing + metrics\n",
    "    with torch.no_grad():\n",
    "        y_predicted = model(X_test)\n",
    "        y_predicted_cls = y_predicted.round()\n",
    "\n",
    "        accuracy = y_predicted_cls.eq(y_test).sum()/float(y_test.shape[0])\n",
    "        print(f'accuracy = {acc:.4f}')\n",
    "    rec = recall_score(y_test, y_predicted_cls)\n",
    "    print(f'recall = {rec:.4f}')\n",
    "    prec = precision_score(y_test, y_predicted_cls)\n",
    "    print(f'precision = {prec:.4f}')\n",
    "    f1 = f1_score(y_test, y_predicted_cls)\n",
    "    print(f'f1-score = {f1:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef8c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_training(data_train, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aed4780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOO_method(model, batch1_data):\n",
    "    batch1_X_test_usc, batch1_y_test = batch1_data.vecs.values, batch1_data.labels.values\n",
    "    batch1_X_test = np.array([np.array(xi) for xi in batch1_X_test_usc])\n",
    "    batch1_X_test = torch.from_numpy(batch1_X_test.astype(np.float32))\n",
    "    batch1_y_test = torch.from_numpy(batch1_y_test.astype(np.float32))\n",
    "    batch1_y_test = batch1_y_test.view(batch1_y_test.shape[0],1)\n",
    "    \n",
    "    d = {'patient': [], 'token as vect': [], 'token as word': [],\\\n",
    "     'og prob': [], 'new prob': [], 'prob diff': []}\n",
    "    df_scores = pd.DataFrame(data=d)\n",
    "\n",
    "    for patient in (range(0, len(batch1_X_test))): #for each note in X_test\n",
    "        note_id = round(patient) \n",
    "        og_pred = model(batch1_X_test[patient]).detach().numpy() #predict original probability\n",
    "        vecs_patient = ((batch1_X_test[patient] != 0).nonzero(as_tuple=True)[0]) # collect all the words that were vectorized (only top 10k in whole corpus)\n",
    "\n",
    "        for tens_pos in vecs_patient: # for each present vec in note\n",
    "            batch1_X_test_pert = batch1_X_test #resets input when loop is iterated\n",
    "            arr = tens_pos.numpy() #to usuable array\n",
    "            word_ = tfidf_tokens[arr] #saves word\n",
    "\n",
    "            clone = torch.clone(batch1_X_test_pert[patient][tens_pos]) # copy vect to print later\n",
    "            batch1_X_test_pert[patient][tens_pos] = 0 #leaves word out\n",
    "\n",
    "            new_pred = model(batch1_X_test_pert[patient]) #model predicts probability of note again\n",
    "            new_pred = new_pred.detach().numpy() #prediction is turned into array\n",
    "\n",
    "            batch1_X_test[patient][tens_pos] = clone #returns vect to input\n",
    "            dict = {'token as word': word_,\\\n",
    "                    'token as vect': clone.item(),\\\n",
    "                    'patient': round(note_id+1),\\\n",
    "                    'og prob': float(og_pred[0]), 'new prob': float(new_pred[0]),\\\n",
    "                    'prob diff': float((og_pred-new_pred)[0])}\n",
    "            df_scores = df_scores.append(dict, ignore_index = True)\n",
    "    \n",
    "    return df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd1f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = LOO_method(model, batch1_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
